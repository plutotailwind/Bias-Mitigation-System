{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OZAF_OKsawG8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZAF_OKsawG8",
        "outputId": "0dc51ead-3156-4415-9812-274678f3f296"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install trl\n",
        "!pip install nltk\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "cAoIxynScjjm",
      "metadata": {
        "id": "cAoIxynScjjm"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "if major_version >= 8:\n",
        "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
        "    !pip install \"unsloth[colab_ampere] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "else:\n",
        "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
        "    !pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "pass\n",
        "\n",
        "!pip install \"git+https://github.com/huggingface/transformers.git\" # Native 4bit loading works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HQwxHi4ofsJw",
      "metadata": {
        "id": "HQwxHi4ofsJw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f2826fd3",
      "metadata": {
        "id": "f2826fd3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b9748368",
      "metadata": {
        "id": "b9748368"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import ssl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a8326ac4",
      "metadata": {
        "id": "a8326ac4",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def LoadModelHUB():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "    tokenizer.padding_side = 'right' # to avoid the future warning\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "    return model, tokenizer\n",
        "'''\n",
        "def formatting_prompts_func(sentences):\n",
        "    out_sentences = []\n",
        "    for sentence in sentences['sentences']:\n",
        "        for text in sentence['sentence']:\n",
        "            out_sentences.append(text)\n",
        "\n",
        "    return out_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4fa102de",
      "metadata": {
        "id": "4fa102de",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def LoadModelUnsloth():\n",
        "    max_seq_length = 2048\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\",\n",
        "            max_seq_length = max_seq_length, dtype=None, load_in_4bit=True)\n",
        "\n",
        "\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "                    model,\n",
        "                    r = 16,\n",
        "                    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\",\n",
        "                        \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "                    lora_alpha = 16,\n",
        "                    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "                    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "                    use_gradient_checkpointing = True,\n",
        "                    random_state = 3407,\n",
        "                    use_rslora = False,  # We support rank stabilized LoRA\n",
        "                    loftq_config = None, # And LoftQ\n",
        "                    )\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3e37a7ae",
      "metadata": {
        "id": "3e37a7ae",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def LoadDataset():\n",
        "    dataset = load_dataset(\"McGill-NLP/stereoset\", \"intersentence\", split='validation')\n",
        "    #dataset = dataset.map(formatting_prompts_func, remove_columns=[f for f in dataset.features if not f == 'sentences'],batched=True)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a6332120",
      "metadata": {
        "id": "a6332120"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "def FineTuneModel(model, tokenizer, dataset):\n",
        "    trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 4,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 2,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "    trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d0cc6012",
      "metadata": {
        "id": "d0cc6012",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def InitNLTK():\n",
        "    try:\n",
        "        _create_unverified_https_context = ssl._create_unverified_context\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    else:\n",
        "        ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2d5083ac",
      "metadata": {
        "id": "2d5083ac",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def GetSynonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    return list(synonyms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "c3f64c5f",
      "metadata": {
        "id": "c3f64c5f",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def InitBiasWords():\n",
        "    '''\n",
        "    Initialize the bias words in a list from a file\n",
        "    '''\n",
        "    bias_words = ['black','bad', 'evil', 'inferior', 'weak', 'worthless', 'disgusting',\n",
        "                'stupid', 'ignorant', 'lazy', 'aggressive', 'dangerous', 'violent', 'criminal',\n",
        "                'savage', 'uncivilized', 'barbaric', 'dirty', 'ugly', 'fat', 'skinny',\n",
        "                'anorexic', 'obese', 'old', 'poor', 'cheap', 'greedy', 'selfish',\n",
        "                'ungrateful', 'primitive', 'backwards', 'uneducated', 'uncultured', 'insane',\n",
        "                'mad', 'crazy', 'lunatic', 'delusional', 'paranoid', 'hysterical']\n",
        "\n",
        "\n",
        "    return bias_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "d565d1b5",
      "metadata": {
        "id": "d565d1b5",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def MitigateBias(text, bias_words):\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        synonyms = GetSynonyms(word)\n",
        "        if word in bias_words:\n",
        "            unbiased_synonyms = [syn for syn in synonyms if syn not in bias_words]\n",
        "            if unbiased_synonyms:\n",
        "                new_word = unbiased_synonyms[0]\n",
        "                new_words.append(new_word)\n",
        "            else:\n",
        "                new_words.append(word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "\n",
        "    return ' '.join(new_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "fc609e3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fc609e3c",
        "lines_to_next_cell": 1,
        "outputId": "b48de1e2-9e92-46a3-8bb8-8504d6eb192a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n    if not bias_words:\\n        output = answers[0]['generated_text']\\n    else:\\n        output = MitigateBias(answers[0]['generated_text'],bias_words)\\n\\n    print(output)\\n\""
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def HandleQuestion(model, tokenizer, dataset, bias_words):\n",
        "\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer = tokenizer)\n",
        "    while True:\n",
        "        question = input(\"Your question: \")\n",
        "        if question=='quit':\n",
        "            print(\"Bye\")\n",
        "            break\n",
        "\n",
        "        print(\"finding a good answer for your question, please wait,...\\n\")\n",
        "\n",
        "\n",
        "        outputs = pipe(question, do_sample=True, max_new_tokens=100,\n",
        "                temperature=0.7, top_k=50, top_p=0.95, num_return_sequences=1)\n",
        "        answers=outputs[0]['generated_text']\n",
        "        print(\"Before mitigation of bias: \")\n",
        "        print(answers)\n",
        "        print(\"After mitigation of bias: \")\n",
        "        print(MitigateBias(answers,bias_words))\n",
        "\n",
        "\n",
        "'''\n",
        "    if not bias_words:\n",
        "        output = answers[0]['generated_text']\n",
        "    else:\n",
        "        output = MitigateBias(answers[0]['generated_text'],bias_words)\n",
        "\n",
        "    print(output)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "5a4a2bc3",
      "metadata": {
        "id": "5a4a2bc3"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    InitNLTK() # to remove the bias words\n",
        "    bias_words = InitBiasWords() # list of bias words present in our dataset\n",
        "    model, tokenizer = LoadModelUnsloth() # Load the mistrel 7b ai model\n",
        "    dataset = LoadDataset() # load our dataset\n",
        "    FineTuneModel(model, tokenizer, dataset)\n",
        "\n",
        "    HandleQuestion(model, tokenizer, dataset, bias_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d37e5aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7d37e5aa",
        "outputId": "e11d5d99-ca17-4f6f-e0b3-321b62d65c41"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
